---
title: "Model_Selection"
author: "Kaija Gahm"
date: "2/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load libraries
```{r}
source("libraries.R")
```

# Load data
```{r}
load("data/outputs/dat3.Rda")
```

# Mixed modeling
Hierarchical mixed model, incorporating pond within clutch and accounting for multiple trials per individual
Dat3: fastest 3 trials per tadpole

## Establish random effect structure
```{r}
mod1 <- lmer(sline_speed_m_s ~ treatment * mass + (1|pond/clutch) + (1|indiv), data = dat3)
summary(mod1)

# removing the pond random effect, because it doesn't tell us anything. In fact, we need to do this to get a non-singular fit.
mod2 <- lmer(sline_speed_m_s ~ treatment * mass + (1|pond:clutch) + (1|indiv), data = dat3)
mod3 <- lmer(sline_speed_m_s ~ treatment * mass + (1|clutch/indiv), data = dat3) # use this syntax, but remember that clutch already accounts for being nested within pond. 

# : specifies that clutch is nested within pond, but without first fitting pond (since we discovered we don't need to)
# / explicitly fits pond first and then clutch.


dat3$predicted_speed <- predict(mod1, re.form = NA, newdata = dat3)
dat3$predicted_speed_withresids <- predict(mod1, re.form = NA, newdata = dat3) + resid(mod1, newdata = dat3)
```

## Check random effect structure
```{r}
# Compare ggplots
dat3 %>% ggplot(aes(x = mass, y = sline_speed_m_s, col = treatment))+
  geom_point()+
  geom_smooth(method = "lm")+
  ylab("speed")+
  ggtitle("Original data")

dat3 %>% ggplot(aes(x = mass, y = predicted_speed, col = treatment))+
  geom_point()+
  geom_smooth(method = "lm")+
  ylab("predicted_speed")+
  ggtitle("Predicted data")

dat3 %>% ggplot(aes(x = mass, y = predicted_speed_withresids, col = treatment))+
  geom_point()+
  geom_smooth(method = "lm")+
  ylab("predicted_speed_withresids")+
  ggtitle("Predicted data with residuals")
```
The similarity between these graphs shows us that we have correctly specified the structure of our random effects


## Figure out main effects
### Investigate correlations
```{r}
#dat3 %>% select(PC1, PC2, size, mass, gs, T_initial, treatment) %>% ggpairs()
```
Size, mass, and gosner stage are all closely correlated. 

## Remove observations that are NA for mass
```{r}
dat3 <- dat3 %>% filter(!is.na(mass))
```

### Remove NA's
```{r}
dat3_nonas <- dat3 %>% filter(!is.na(T_initial))
save(dat3_nonas, file = "data/outputs/dat3_nonas.Rda")
```

### Standardize variables
```{r}
# Function to calculate z scores
standardize <- function(vector){
  mu <- mean(vector)
  sd <- sd(vector)
  z_vector <- (vector-mu)/sd
  return(z_vector)
}

dat3_nonas <- dat3_nonas %>% mutate_at(.vars = c("PC1", "PC2", "mass", "size", "gs", "T_initial"), standardize)
```

## Make some biological decisions
In all of the tests that we ran, PC1 and PC2 were kind of equivalent (but with slightly more effect of PC2 than PC1), and mass and size were also basically equivalent. Therefore, we choose:
1. PC2 over PC1
2. mass over size: since they're pretty much the same anyway, this shouldn't change the results. Mass was calculated by weighing the tadpoles, whereas size was calculated from the same landmarks used to calculate the shape PC's: seems more indirect, so we'll go with mass. Mass is also more of a real biological phenomenon. Size would basically end up being the area of a cross section of the tadpole, which is probably not as important for physiological processes as mass. The two are also highly correlated, so we don't want both in the model.

## Model selection
### Fit full model
```{r}
full <- lmer(sline_speed_m_s ~ PC2*mass*gs*treatment + T_initial + (1|clutch/indiv),
             data = dat3_nonas, na.action = "na.fail", REML = F)
vif(full) # check variance inflation factors: kind of high, but we're also going to be removing some of the interaction terms. Doesn't look too bad for the individual terms, especially because we already know that mass and gosner stage are correlated.

#dredge_full <- dredge(full)
#save(dredge_full, file = "data/outputs/dredge_full.Rda")

# Look at top five, see if the actual results for treatment effect are changing. If not, then we just know that we're accounting for the variables of interest. Provide table in supplements. 
```

### Fit top five models
```{r}
load("data/outputs/dredge_full.Rda") # load the dredge output
dredge_full[1:5,] # take a look at the top five models

m1 <- lmer(sline_speed_m_s ~ mass*treatment + gs + T_initial + 
             (1 | clutch/indiv), data = dat3_nonas, REML = F, 
    na.action = "na.fail")

m2 <- lmer(sline_speed_m_s ~ mass*treatment + PC2 + gs + T_initial + 
             (1 | clutch/indiv), data = dat3_nonas, REML = F, 
    na.action = "na.fail")

m3 <- lmer(sline_speed_m_s ~ mass*treatment + gs*mass + T_initial + 
             (1 | clutch/indiv), data = dat3_nonas, REML = F, 
    na.action = "na.fail")

m4 <- lmer(sline_speed_m_s ~ treatment*gs + T_initial + 
             (1 | clutch/indiv), data = dat3_nonas, REML = F, 
    na.action = "na.fail")

m5 <- lmer(sline_speed_m_s ~ mass*treatment + gs*mass + PC2 + T_initial + 
             (1 | clutch/indiv), data = dat3_nonas, REML = F, na.action = "na.fail")

# all of these have singular fits! Is that a problem?
```

### Create new df
```{r}
newdf <- dat3_nonas %>% select(treatment, clutch, indiv)
newdf_means <- dat3_nonas %>% summarize(mass = mean(mass),
                                        gs = mean(gs),
                                        T_initial = mean(T_initial),
                                        PC2 = mean(PC2))
newdf[,4:7] <- newdf_means
newdf <- newdf %>% mutate(model1 = predict(m1, newdf),
                          model2 = predict(m2, newdf),
                          model3 = predict(m3, newdf),
                          model4 = predict(m4, newdf),
                          model5 = predict(m5, newdf))

df_preds <- newdf %>% select(treatment, model1, model2, model3, model4, model5) %>% 
  pivot_longer(cols = c(model1, model2, model3, model4, model5), 
               names_to = "model",
               values_to = "predicted_speed")

df_preds %>% ggplot(aes(x = treatment, y = predicted_speed, fill = treatment))+
  geom_boxplot()+
  facet_wrap(~model)
```
















## Check residuals
```{r}
dat3_nonas <- dat3_nonas %>% mutate(resid = resid(mod_gs))

dat3_nonas %$% qqnorm(resid)
plot(mod_mass) # looks roughly normal, but just to make sure, we'll do individual plots

dat3_nonas %>% ggplot(aes(x = PC1, y = resid))+
  geom_point() +
  facet_wrap(~treatment)

dat3_nonas %>% ggplot(aes(x = gs, y = resid))+
  geom_point() +
  facet_wrap(~treatment) # the gosner stage data violates the equal variance assumption

dat3_nonas %>% ggplot(aes(x = T_initial, y = resid))+
  geom_point() +
  facet_wrap(~treatment)

dat3_nonas %>% ggplot(aes(x = resid, col = treatment))+
  geom_density(aes(fill = treatment), alpha = 0.5)
```


## Add variance structure
```{r}
# this is how we fitted our original model, mod_gs
mod_gs <- lmer(sline_speed_m_s ~ gs*treatment*PC1 + T_initial + (1 | clutch/indiv), data = dat3_nonas)

mod_nostruct <- lme(sline_speed_m_s ~ gs*treatment*PC1 + T_initial, 
         random = ~ 1 | clutch/indiv, 
         data = dat3_nonas)
summary(mod_nostruct)

mod_varfixed <- lme(sline_speed_m_s ~ gs*treatment*PC1 + T_initial, 
         random = ~ 1 | clutch/indiv,
         weights = varFixed(~gs),
         data = dat3_nonas)
summary(mod_varfixed)

# this one does not converge
# mod_varident <- lme(sline_speed_m_s ~ gs*treatment*PC1 + T_initial, 
#          random = ~ 1 | clutch/indiv,
#          weights = varIdent(form = ~ 1 | factor(as.character(gs))),
#          data = dat3_nonas) # does not converge

mod_varpower <- lme(sline_speed_m_s ~ gs*treatment*PC1 + T_initial, 
         random = ~ 1 | clutch/indiv,
         weights = varPower(form = ~ gs),
         data = dat3_nonas)
summary(mod_varpower) # but we shouldn't use this, since gs can take a value of 0 once scaled.

mod_varexp <- lme(sline_speed_m_s ~ gs*treatment*PC1 + T_initial, 
         random = ~ 1 | clutch/indiv,
         weights = varExp(form = ~ gs),
         data = dat3_nonas)
summary(mod_varexp)

mod_varconstpower <- lme(sline_speed_m_s ~ gs*treatment*PC1 + T_initial, 
         random = ~ 1 | clutch/indiv,
         weights = varConstPower(form = ~ gs),
         data = dat3_nonas)
summary(mod_varconstpower)


anova(mod_nostruct, mod_varfixed, mod_varexp, mod_varconstpower) # varexp seems to be the best, but not by a particularly large margin.
```

If var_exp is the best variance structure, we should re-do the models 
## Re-do models with variance structure
### New full model with mass and gs
```{r}
newfull_mass_gs <- lmer(sline_speed_m_s ~ PC1*PC2*mass*treatment + gs + T_initial + (1|clutch/indiv), data = dat3_nonas, na.action = "na.fail", REML = FALSE) 

#newfull_mass_gs_best <- dredge(newfull_mass_gs)[1,]
#save(newfull_mass_gs_best, file = "data/outputs/newfull_mass_gs_best.Rda")
load("data/outputs/newfull_mass_gs_best.Rda")
```

### New best model
```{r}
newfull_mass_gs_best
mod_mass_gs <- lmer(sline_speed_m_s ~ mass*treatment + gs + T_initial + (1 | clutch/indiv), data = dat3_nonas) # the best model failed to converge. What do we do??
```


## Select model
```{r}
consensus_mod <- mod_gs
summary(mod_gs)
save(consensus_mod, file = "data/outputs/consensus_mod.Rda")
```
